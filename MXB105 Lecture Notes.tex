%!TEX program = xelatex
\documentclass{article}
\usepackage{LaTeX-Submodule/template}

% Additional packages & macros
\usepackage{multicol}

% Header and footer
\newcommand{\className}{Calculus and Differential Equations}
\newcommand{\classTime}{Semester 2, 2021}
\newcommand{\classInstructorName}{Dr Vivien Challis}

\fancyhead[L]{\className}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\className}}
        \texorpdfstring{\\}{ }
        \texorpdfstring{\vspace{0.1in}}{ }
        \normalsize{\classTime}
        \texorpdfstring{\\}{ }
        \texorpdfstring{\vspace{0.2in}}{ }
        \normalsize\textit{\classInstructorName}
        \texorpdfstring{\\}{ }
    \end{center}
    \begin{center}
        \textsc{Rohan Boas \quad\quad Tarang Janawalkar}
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Integration Techniques}
\subsection{Table of Derivatives}
Let $f(x)$ be a function, and $a\in\mathbb{R}$ be a constant.
\begin{table}[H]
    \renewcommand*{\arraystretch}{1.5}
    \centering
    \begin{tabular}{>{$}c<{$} | >{$}c<{$}}
        \toprule
        f                                                                        & \dv{f}{x}                                      \\
        \midrule
        x^a                                                                      & a x^{a-1}                                      \\
        \sqrt{x}                                                                 & \displaystyle \frac{1}{2\sqrt{x}}              \\
        a^x                                                                      & \ln{\left( a \right)} a^x                      \\
        \e^x                                                                     & \e^x                                           \\
        \log_a{\left( x \right)}, \: a\in \mathbb{R}\backslash\left\{ 0 \right\} & \displaystyle \frac{1}{x\ln{\left( a \right)}} \\[8pt]
        \ln{\left( x \right)}                                                    & \displaystyle \frac{1}{x}                      \\[5pt]
        \bottomrule
    \end{tabular}
    \begin{tabular}{>{$}c<{$} | >{$}c<{$}}
        \toprule
        f                                & \dv{f}{x}                                              \\
        \midrule
        a                                & 0                                                      \\
        x                                & 1                                                      \\
        a_1 u(x) \pm a_2 v(x)            & \displaystyle a_1\dv{u}{x} \pm a_2\dv{v}{x}            \\[8pt]
        u(x)v(x)                         & \displaystyle \dv{u}{x}v + u\dv{v}{x}                  \\[10pt]
        \displaystyle \frac{u(x)}{v(x)}  & \displaystyle \frac{\dv{u}{x}v - u\dv{v}{x}}{{v(x)}^2} \\[8pt]
        u\bigl( v\left( x \right) \bigr) & \displaystyle \dv{u}{v}\dv{v}{x}                       \\[5pt]
        \bottomrule
    \end{tabular}
\end{table}
\begin{table}[H]
    \renewcommand*{\arraystretch}{1.5}
    \centering
    \begin{tabular}{>{$}c<{$} | >{$}c<{$}}
        \toprule
        f                       & \dv{f}{x}                                        \\
        \midrule
        \sin{\left( ax \right)} & a\cos{\left( ax \right)}                         \\
        \cos{\left( ax \right)} & -a\sin{\left( ax \right)}                        \\
        \tan{\left( ax \right)} & a\sec^2{\left( ax \right)}                       \\
        \cot{\left( ax \right)} & -a\csc^2{\left( ax \right)}                      \\
        \sec{\left( ax \right)} & a\sec{\left( ax \right)}\tan{\left( ax \right)}  \\
        \csc{\left( ax \right)} & -a\csc{\left( ax \right)}\cot{\left( ax \right)} \\[5pt]
        \bottomrule
    \end{tabular}
    \begin{tabular}{>{$}c<{$} | >{$}c<{$}}
        \toprule
        f                          & \dv{f}{x}                                   \\
        \midrule
        \arcsin{\left( ax \right)} & \displaystyle  \frac{a}{\sqrt{1-a^2x^2}}    \\[8pt]
        \arccos{\left( ax \right)} & \displaystyle -\frac{a}{\sqrt{1-a^2x^2}}    \\[8pt]
        \arctan{\left( ax \right)} & \displaystyle  \frac{a}{1+a^2x^2}           \\[8pt]
        \arccot{\left( ax \right)} & \displaystyle -\frac{a}{1+a^2x^2}           \\[8pt]
        \arcsec{\left( ax \right)} & \displaystyle  \frac{1}{x\sqrt{a^2x^2 - 1}} \\[8pt]
        \arccsc{\left( ax \right)} & \displaystyle -\frac{1}{x\sqrt{a^2x^2 - 1}} \\[8pt]
        \bottomrule
    \end{tabular}
\end{table}
\begin{table}[H]
    \renewcommand*{\arraystretch}{1.5}
    \centering
    \hspace*{-1cm}
    \begin{tabular}{>{$}c<{$} | >{$}c<{$}}
        \toprule
        f                        & \dv{f}{x}                                         \\
        \midrule
        \sinh{\left( ax \right)} & a\cosh{\left( ax \right)}                         \\
        \cosh{\left( ax \right)} & a\sinh{\left( ax \right)}                         \\
        \tanh{\left( ax \right)} & a\sech^2{\left( ax \right)}                       \\
        \coth{\left( ax \right)} & -a\csch^2{\left( ax \right)}                      \\
        \sech{\left( ax \right)} & -a\sech{\left( ax \right)}\tan{\left( ax \right)} \\
        \csch{\left( ax \right)} & -a\csch{\left( ax \right)}\cot{\left( ax \right)} \\[5pt]
        \bottomrule
    \end{tabular}
    \begin{tabular}{>{$}c<{$} | >{$}c<{$}}
        \toprule
        f                           & \dv{f}{x}                                \\
        \midrule
        \arcsinh{\left( ax \right)} & \displaystyle  \frac{a}{\sqrt{1+a^2x^2}} \\[8pt]
        \arccosh{\left( ax \right)} & \displaystyle  \frac{a}{\sqrt{1-a^2x^2}} \\[8pt]
        \arctanh{\left( ax \right)} & \displaystyle  \frac{a}{1-a^2x^2}        \\[8pt]
        \bottomrule
    \end{tabular}
    \begin{tabular}{>{$}c<{$} | >{$}c<{$}}
        \toprule
        f                           & \dv{f}{x}                                                             \\
        \midrule
        \arccoth{\left( ax \right)} & \displaystyle  \frac{a}{1-a^2x^2}                                     \\[8pt]
        \arcsech{\left( ax \right)} & \displaystyle -\frac{1}{a\left( 1+ax \right)\sqrt{\frac{1-ax}{1+ax}}} \\[8pt]
        \arccsch{\left( ax \right)} & \displaystyle -\frac{1}{ax^2\sqrt{1+\frac{1}{a^2x^2}}}                \\[8pt]
        \bottomrule
    \end{tabular}
    \hspace*{-1cm}
    \caption{Derivatives of Elementary Functions}
\end{table}
\subsection{Trigonometric Identities}
\subsubsection{Pythagorean Identities}
\begin{equation*}
    \sin^2{\left( x \right)} + \cos^2{\left( x \right)} = 1
\end{equation*}
Dividing by either the sine or cosine function gives:
\begin{align*}
    \tan^2{\left( x \right)} + 1 & = \sec^2{\left( x \right)} \\
    1 + \cot^2{\left( x \right)} & = \csc^2{\left( x \right)}
\end{align*}
\subsubsection{Double-Angle Identities}
\begin{align*}
    \sin{\left( 2x \right)} & = 2\sin{\left( x \right)}\cos{\left( x \right)}              & \csc{\left( 2x \right)} & = \frac{\sec{\left( x \right)}\csc{\left( x \right)}}{2}                                                     \\[5pt]
    \cos{\left( 2x \right)} & = \cos^2{\left( x \right)} - \sin^2{\left( x \right)}        & \sec{\left( 2x \right)} & = \frac{\sec^2{\left( x \right)}\csc^2{\left( x \right)}}{\csc^2{\left( x \right)}-\sec^2{\left( x \right)}} \\[5pt]
    \tan{\left( 2x \right)} & = \frac{2\tan{\left( x \right)}}{1-\tan^2{\left( x \right)}} & \cot{\left( 2x \right)} & = \frac{\cot^2{\left( x \right)}-1}{2\cot{\left( x \right)}}
\end{align*}
\subsubsection{Power Reducing Identities}
\begin{align*}
    \sin^2{\left( x \right)} & = \frac{1-\cos{\left( 2x \right)}}{2}                         & \csc^2{\left( x \right)} & = \frac{2}{1-\cos{\left( 2x \right)}}                         \\[5pt]
    \cos^2{\left( x \right)} & = \frac{1+\cos{\left( 2x \right)}}{2}                         & \sec^2{\left( x \right)} & = \frac{2}{1+\cos{\left( 2x \right)}}                         \\[5pt]
    \tan^2{\left( x \right)} & = \frac{1-\cos{\left( 2x \right)}}{1+\cos{\left( 2x \right)}} & \cot^2{\left( x \right)} & = \frac{1+\cos{\left( 2x \right)}}{1-\cos{\left( 2x \right)}}
\end{align*}
\subsection{Partial Fractions}
\begin{definition}[Partial Fraction Decomposition]
    \textbf{Partial fraction decomposition} is a \linebreak method where a rational function $\displaystyle \frac{P(x)}{Q(x)}$ is rewritten as a sum of fraction.
\end{definition}
\begin{table}[H]
    \renewcommand*{\arraystretch}{1.5}
    \centering
    \begin{tabular}{c | c}
        \toprule
        \textbf{Factor in denominator}                  & \textbf{Term in partial fraction decomposition}                                                                                                 \\
        \midrule
        $ax+b$                                          & $\displaystyle \frac{A}{ax+b}$                                                                                                                  \\[10pt]
        $\left(ax+b\right)^k, \: k \in \mathbb{N}$      & $\displaystyle \frac{A_1}{ax+b} + \frac{A_2}{\left( ax+b \right)^2} + \cdots + \frac{A_k}{\left( ax+b \right)^k}$                               \\[10pt]
        $ax^2+bx+c$                                     & $\displaystyle \frac{A}{ax^2+bx+c}$                                                                                                             \\[10pt]
        $\left(ax^2+bx+c\right)^k, \: k \in \mathbb{N}$ & $\displaystyle \frac{A_1x+B_1}{ax^2+bx+c} + \frac{A_2x+B_2}{\left( ax^2+bx+c \right)^2} + \cdots + \frac{A_kx+B_k}{\left( ax^2+bx+c \right)^k}$ \\[10pt]
        \bottomrule
    \end{tabular}
    \caption{Partial Fraction Forms}
\end{table}
\subsection{Integration by Parts}
\begin{theorem}
    \begin{equation*}
        \int u \dd{v} = uv - \int v \dd{u}
    \end{equation*}
\end{theorem}
\subsection{Integration by Substitution}
\begin{theorem}
    \begin{equation*}
        \int f\bigl(g\left( x \right)\bigr)\dv{g(x)}{x} \dd{x} = \int f(u) \dd{u}, \: \text{where } u = g(x)
    \end{equation*}
\end{theorem}
\subsection{Trigonometric Substitutions}
\begin{table}[H]
    \renewcommand*{\arraystretch}{1.5}
    \centering
    \begin{tabular}{>{$}c<{$} | >{$}c<{$} >{$}c<{$} | >{$}c<{$}}
        \toprule
        \text{\textbf{Form}}      & \text{\textbf{Substitution}}                           & \text{\textbf{Result}}           & \text{\textbf{Domain}}                                                             \\
        \midrule
        \left(a^2-b^2x^2\right)^n & \displaystyle x=\frac{a}{b}\sin{\left( \theta \right)} & a^2\cos^2{\left( \theta \right)} & \theta\in \left[ -\frac{\pi}{2},\: \frac{\pi}{2} \right]                           \\[8pt]
        \left(a^2+b^2x^2\right)^n & \displaystyle x=\frac{a}{b}\tan{\left( \theta \right)} & a^2\sec^2{\left( \theta \right)} & \theta\in \left( -\frac{\pi}{2},\: \frac{\pi}{2} \right)                           \\[8pt]
        \left(b^2x^2-a^2\right)^n & \displaystyle x=\frac{a}{b}\sec{\left( \theta \right)} & a^2\tan^2{\left( \theta \right)} & \theta\in \left[ 0,\: \frac{\pi}{2} \right) \cup \left(\frac{\pi}{2},\: \pi\right] \\
        \bottomrule
    \end{tabular}
    \caption{Trigonometric substitutions for various forms.}
\end{table}
\newpage
\section{Limits, Continuity and Differentiability}
\subsection{Limits}
\begin{theorem}[Limits]
    $\displaystyle\lim_{x\to x_0} f(x)$ exists if and only if
    $\displaystyle\lim_{x\to {x_0}^+} f(x)$ and $\displaystyle\lim_{x\to {x_0}^-} f(x)$
    exist and are equal.
\end{theorem}
\begin{definition}[Finite limits using the $\varepsilon$-$\delta$ definition]
    \begin{equation*}
        \lim_{x\to x_0} f(x) = L \iff \forall\varepsilon>0: \exists\delta>0: \forall x \in I: 0<\abs{x-x_0}<\delta \implies \abs{f(x)-L}<\varepsilon
    \end{equation*}
\end{definition}
\begin{theorem}[L'Hôpital's Rule]
    For two differentiable functions $f(x)$ and $g(x)$.
    If $\displaystyle \lim_{x\to x_0}f(x)=\lim_{x\to x_0}g(x)=0$,
    or $\displaystyle \lim_{x\to x_0}f(x)=\displaystyle \lim_{x\to x_0}g(x)=\pm\infty$,
    then
    $\lim_{x\to x_0}\frac{f(x)}{g(x)} = \lim_{x\to x_0}\frac{f'(x)}{g'(x)}$.
\end{theorem}
\subsection{Continuity}
\begin{theorem}[Continuity at a Point]
    $f(x)$ is continuous at $c$ iff $\displaystyle \lim_{x\to c} f(x) = f(c)$.
\end{theorem}
\begin{theorem}[Continuity over an Interval]
    $f(x)$ is continuous on $I$ if $f(x)$ is continuous for all $x\in I$.
    \begin{itemize}
        \item $f(x)$ is continuous on $I:\left( a,\:b \right)$ if it is continuous
              for all $x\in I$.
        \item $f(x)$ is continuous on $I:\left[ a,\:b \right]$ if it is continuous
              for all $x\in I$, but only right continuous at $a$ and left continuous at $b$.
    \end{itemize}
    If $f(x)$  is continuous on $\left(-\infty,\:\infty\right)$, $f(x)$ is
    continuous everywhere.
\end{theorem}
\begin{theorem}[Intermediate Value Theorem]
    If $f(x)$ is continuous on $I:\left[ a, \: b \right]$ and $c$ is any number
    between $f(a)$ and $f(b)$, inclusive, then there exists an $x\in I$ such
    that $f(x)=c$.
\end{theorem}
\subsection{Differentiability}
\begin{theorem}[Differentiability]
    $f(x)$ is differentiable at $x=x_0$ iff
    \begin{equation*}
        \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}
    \end{equation*}
    exists. When this limit exists, it defines the derivative
    \begin{equation*}
        \left.\dv{f}{x}\right|_{x=x_0} = \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)}{h}
    \end{equation*}
\end{theorem}
\begin{theorem}
    $f(x)$ is differentiable on $I$ if $f(x)$ is differentiable for all
    $x_0\in I$.
\end{theorem}
\begin{theorem}
    Differentiability implies continuity.
\end{theorem}
\begin{theorem}[Mean Value Theorem]
    If $f(x)$ is continuous on $I:\left[ a,\:b \right]$ and differentiable on
    $I$, then there exists a point $c\in I$ such that
    \begin{equation*}
        \left.\dv{f}{x}\right|_{x=c}=\frac{f(b)-f(a)}{b-a}
    \end{equation*}
\end{theorem}
\newpage
\section{Definite Integrals}
\begin{theorem}
    If $f(x)$ is continuous on an interval $I:\left[ a,\:b \right]$, then the
    net signed area $A$ between the graph of $f(x)$ and the interval $I$ is
    \begin{equation*}
        A = \int_a^b f(x) \dd{x}
    \end{equation*}
\end{theorem}
\begin{tcolorboxlarge}[title={Properties of Definite Integrals}]
    \begin{theorem}
        Suppose that $f(x)$ and $g(x)$ are continuous on the interval $I$,
        with $a,\:b,\:c\in I$, with $a < c < b$, and $k\in\mathbb{R}$ then
        \begin{enumerate}[label=\normalfont\alph*)]
            \item $\displaystyle\int_a^a f(x) \dd{x} = 0$.
            \item $\displaystyle\int_a^b f(x) \dd{x} = -\int_b^a f(x) \dd{x}$.
            \item $\displaystyle\int_a^b kf(x) \dd{x} = k\int_a^b f(x) \dd{x}$.
            \item $\displaystyle\int_a^b \bigl(f(x) \pm g(x)\bigr) \dd{x} = \int_a^b f(x) \dd{x} \pm \int_a^b g(x) \dd{x}$.
            \item $\displaystyle\int_a^b f(x) \dd{x} = \int_a^c f(x) \dd{x} + \int_c^b f(x) \dd{x}$.
        \end{enumerate}
    \end{theorem}
\end{tcolorboxlarge}
\subsection{Riemann Sums}
\begin{theorem}\label{theorem:1d_riemann_sums}
    Let $A$ be the area under $f(x)$ on the interval $\left[ a,\:b \right]$,
    then
    \begin{equation*}
        \int_a^b f(x) \dd{x} = \lim_{\max{\Delta x_k}\to 0} \sum_{k=1}^n f(x_k) \Delta x_k
    \end{equation*}
    where $n$ is the number of rectangles, $x_k$ is the centre of the rectangle
    $k$, and $\Delta x_k$ is the width of the rectangle $k$. If every rectangle
    has the same width, then
    \begin{equation*}
        \forall k:\Delta x_k = \frac{b-a}{n}
    \end{equation*}
\end{theorem}
\subsection{Fundamental Theorem of Calculus}
The fundamental theorem of calculus provides a logical connection between
infinite series (definite integrals) and antiderivatives
(indefinite integrals).
\begin{theorem}[The Fundamental Theorem of Calculus: Part 1]
    If $f(x)$ is continuous on $\left[ a,\:b \right]$ and $F$ is any
    antiderivative of $f$ on $\left[ a,\:b \right]$ then
    \begin{equation*}
        \int_a^b f(x)\dd{x} = F(b) - F(a)
    \end{equation*}
    Equivalently
    \begin{equation*}
        \int_a^b \dv{x}F(x) \dd{x} = F(b) - F(a) \equiv \left.F(x)\right|_a^b
    \end{equation*}
\end{theorem}
\begin{theorem}[The Fundamental Theorem of Calculus: Part 2]
    If $f(x)$ is continuous on $I$ then it has an antiderivative on $I$. In
    particular, if $a\in I$, then the function $F$ defined by
    \begin{equation*}
        F(x) = \int_a^x f(t)\dd{t}
    \end{equation*}
    is an antiderivative of $f(x)$. That is,
    \begin{equation*}
        \dv{x}F(x) = f(x) \equiv \dv{x}\int_a^x f(t) \dd{t} = f(x)
    \end{equation*}
\end{theorem}
\begin{theorem}
    Differentiation and integration are inverse operations.
\end{theorem}
\newpage
\subsection{Taylor and Maclaurin Polynomials}
\begin{theorem}[Taylor Polynomials]
    If $f(x)$ is an $n$ differentiable function at $x_0$, then the $n$th degree
    Taylor polynomial for $f(x)$ near $x_0$, is given by
    \begin{equation*}
        f(x) \approx p_n(x) = \sum_{k=0}^n \frac{f^{\left( k \right)}(x_0)}{k!} \left( x-x_0 \right)^k
    \end{equation*}
\end{theorem}
\begin{theorem}[Maclaurin Polynomials]
    Evaluating a Taylor polynomial near $0$, gives the $n$th degree Maclaurin
    polynomial for $f(x)$
    \begin{equation*}
        f(x) \approx p_n(x) = \sum_{k=0}^n \frac{f^{\left( k \right)}(0)}{k!} x^k
    \end{equation*}
\end{theorem}
\begin{theorem}[Error in Approximation]
    Let $R_n(x)$ denote the difference between $f(x)$ and its $n$th Taylor
    polynomial, that is
    \begin{equation*}
        R_n(x) = f(x) - p_n(x) = f(x) - \sum_{k=0}^n \frac{f^{\left( k \right)}(x_0)}{k!} \left( x-x_0 \right)^k = \frac{f^{\left( n+1 \right)}(s)}{\left( n+1 \right)!} \left( x-x_0 \right)^{n+1}
    \end{equation*}
    where $s$ is between $x_0$ and $x$.
\end{theorem}
\newpage
\section{Taylor and Maclaurin Series}
\subsection{Infinite Series}
\begin{definition}[Taylor Series]
    If $f(x)$  has derivatives of all orders at $x_0$, then the Taylor series
    for $f(x)$ about $x=x_0$ is given by
    \begin{equation*}
        f(x) = \sum_{n=0}^{\infty} \frac{f^{\left( n \right)}(x_0)}{n!}\left( x-x_0 \right)^n
    \end{equation*}
\end{definition}
\begin{definition}[Maclaurin Series]
    If a Taylor series is centred at $x_0=0$, it is called a Maclaurin series,
    defined by
    \begin{equation*}
        f(x) = \sum_{n=0}^{\infty} \frac{f^{\left( n \right)}(0)}{n!} x^n
    \end{equation*}
\end{definition}
\begin{definition}[Power Series]
    Both Taylor and Maclaurin series are examples of \textbf{power series},
    which are defined as follows
    \begin{equation*}
        \sum_{n=0}^{\infty} c_n\left( x-x_0 \right)^n
    \end{equation*}
\end{definition}
\subsection{Convergence}
\begin{theorem}[Convergence of a Taylor Series]
    The equality
    \begin{equation*}
        f(x) = \sum_{n=0}^{\infty} \frac{f^{\left( n \right)}(x_0)}{n!} \left( x-x_0 \right)^n
    \end{equation*}
    holds at a point $x$ iff
    \begin{align*}
        \lim_{n\to\infty} \left[ f(x) - \sum_{k=0}^{n} \frac{f^{\left( k \right)}(x_0)}{k!} \left( x-x_0 \right)^k \right] & = 0 \\
        \lim_{n\to\infty} R_n(x)                                                                                           & = 0
    \end{align*}
\end{theorem}
\begin{definition}[Interval of Convergence]
    The interval of convergence for a power series is the set of $x$ values for
    which that series converges.
\end{definition}
\begin{definition}[Radius of Convergence]
    The radius of convergence $R$ is a nonnegative real number or $\infty$ such
    that
    a power series converges if
    \begin{equation*}
        \abs{x - a} < R
    \end{equation*}
    and diverges if
    \begin{equation*}
        \abs{x - a} > R
    \end{equation*}
    The behaviour of the power series on the boundary, that is, where
    $\abs{x - a} = R$, can be determined by substituting $x = R + a$ and
    $x = -R + a$ into the series, for the upper and lower boundaries, respectively.
\end{definition}
\subsection{Convergence Tests}
For any power series of the form $\displaystyle\sum_{i=i_0}^\infty a_i$.
\begin{tcolorboxlarge}[title={Alternating Series}]
    \textbf{Conditions} $a_i = \left( -1 \right)^i b_i$ or
    $a_i = \left( -1 \right)^{i+1} b_i$. $b_i>0$.
    \begin{equation*}
        \text{Is $b_{i+1}\leqslant b_i$ \& $\lim_{i\to\infty}b_i=0$?}\:
        \begin{cases}
            \text{YES} & \text{$\sum a_i$ Converges} \\
            \text{NO}  & \text{Inconclusive}
        \end{cases}
    \end{equation*}
\end{tcolorboxlarge}
\begin{tcolorboxlarge}[title={Ratio Test}]
    \begin{equation*}
        \text{Is $\lim_{i\to\infty}\abs{\frac{a_{i+1}}{a_i}} < 1$?}\:
        \begin{cases}
            \text{YES} & \text{$\sum a_i$ Converges} \\
            \text{NO}  & \text{$\sum a_i$ Diverges}
        \end{cases}
    \end{equation*}
    The ratio test is inconclusive if
    $\displaystyle \lim_{i\to\infty}\frac{a_{i+1}}{a_i} = 1$.
\end{tcolorboxlarge}
\subsection{Table of Maclaurin Series}
\begin{table}[H]
    \centering
    \begin{tabular}{c | c | c}
        \toprule
        \textbf{Function}                       & \textbf{Series}                                                                               & \textbf{Interval of Convergence} \\
        \midrule
        $\e^{x}$                                & $\displaystyle \sum_{n=0}^{\infty} \frac{x^n}{n!}$                                            & $-\infty < x < \infty$           \\[14pt]
        $\sin{\left( x \right)}$                & $\displaystyle \sum_{n=0}^{\infty} \left( -1 \right)^n \frac{x^{2n+1}}{\left( 2n+1 \right)!}$ & $-\infty < x < \infty$           \\[14pt]
        $\cos{\left( x \right)}$                & $\displaystyle \sum_{n=0}^{\infty} \left( -1 \right)^n \frac{x^{2n}}{\left( 2n \right)!}$     & $-\infty < x < \infty$           \\[14pt]
        $\displaystyle \frac{1}{1-x}$           & $\displaystyle \sum_{n=0}^{\infty} x^n$                                                       & $-1 < x < 1$                     \\[14pt]
        $\displaystyle \frac{1}{1+x^2}$         & $\displaystyle \sum_{n=0}^{\infty} \left( -1 \right)^n x^{2n}$                                & $-1 < x < 1$                     \\[14pt]
        $\displaystyle \ln{\left( 1+x \right)}$ & $\displaystyle \sum_{n=0}^{\infty} \left( -1 \right)^{n+1} \frac{x^n}{n}$                     & $-1 < x \leq 1$                  \\[14pt]
        \bottomrule
    \end{tabular}
    \caption{Maclaurin series of common functions.}
    % \label{}
\end{table}
\newpage
\section{Multivariable Calculus}
\subsection{Multivariable Functions}
\begin{definition}
    A function is multivariable if its domain consists of several variables. In
    the reals, these functions are defined
    \begin{equation*}
        f:\mathbb{R}^n\to\mathbb{R}
    \end{equation*}
\end{definition}
\subsection{Level Curves}
\begin{definition}
    The level curves or \textit{contour curves} of a function of two variables are
    curves along which the function has a constant value.
    \begin{equation*}
        L_c\left( f \right) = \bigl\{ \left( x,\: y \right) : f\left(x,\: y\right) = c\bigr\}
    \end{equation*}
    The level curves of a function can be determined by substituting $z=c$, and
    solving for $y$.
\end{definition}
\subsection{Limits and Continuity}
\begin{definition}[Finite Limit of Multivariable Functions using the
        $\varepsilon$-$\delta$ Definition]
    \begin{align*}
        \lim_{(x_1,\: \ldots,\: x_n)\to (c_1,\: \ldots,\: c_n)} f(x_1,\: \ldots,\: x_n) = L
                                                    & \iff\forall\varepsilon>0: \exists\delta>0: \forall (x_1,\: \ldots,\: x_n) \in I: \\
        0<\abs{x_1-c_1,\: \ldots,\: x_n-c_n}<\delta & \implies \abs{f(x_1,\: \ldots,\: x_n)-L}<\varepsilon
    \end{align*}
\end{definition}
\begin{theorem}[Limits along Smooth Curves]
    If $f(x,\: y) \to L$ as $(x,\: y) \to (x_0,\: y_0)$, then
    $\displaystyle \lim_{(x,\: y) \to (x_0,\: y_0)} = L$ along any smooth
    curve.
\end{theorem}
\begin{theorem}[Existence of a Limit]
    If the limit of $f(x,\: y)$ changes along different smooth curves, then
    $\displaystyle \lim_{(x,\: y) \to (x_0,\: y_0)}$ does not exist.
\end{theorem}
\begin{theorem}[Continuity of Multivariable Functions]
    A function $f(x_1,\: \ldots,\: x_n)$ is continuous at
    $(c_1,\: \ldots,\: c_n)$ iff
    \begin{equation*}
        \lim_{(x_1,\: \ldots,\: x_n)\to (c_1,\: \ldots,\: c_n)} f(x_1,\: \ldots,\: x_n) = f(c_1,\: \ldots,\: c_n)
    \end{equation*}
\end{theorem}
Recognising continuous functions:
\begin{itemize}
    \item A sum, difference or product of continuous functions is continuous.
    \item A quotient of continuous functions is continuous except where the
          denominator is zero.
    \item A composition of continuous functions is continuous.
\end{itemize}
\subsection{Partial Derivatives}
\begin{definition}[Partial Differentiation]
    The partial derivative of a multivariable function is its derivative with
    respect to one of those variables, while the others are held constant.
    \begin{equation*}
        \pdv{f}{x_i} = \lim_{h \to 0} \frac{f(x_1,\: \ldots,\: x_{i-1},\: x_i+h,\: x_{i+1},\: \ldots,\: x_n) - f(x_1,\: \ldots,\: x_n)}{h}
    \end{equation*}
\end{definition}
\subsection{The Gradient Vector}
\begin{definition}
    Let $\symbf{\nabla}$, pronounced ``del'', denote the vector differential
    operator defined as follows
    \begin{equation*}
        \symbf{\nabla} = \mqty[\partial_{x_1} \\ \partial_{x_2} \\ \vdots \\ \partial_{x_n}]
    \end{equation*}
\end{definition}
\subsection{Multivariable Chain Rule}
\begin{definition}
    Let $f=f\bigl(\symbfit{x}(t_1,\: \ldots,\: t_n)\bigr)$ be the composition of $f$ with
    $\symbfit{x}=\mqty[x_1 & \cdots & x_m]$, then the partial derivative of $f$
    with respect to $t_i$ is given by
    \begin{equation*}
        \pdv{f}{t_i} = \symbf{\nabla}f \cdot \partial_{t_i}\symbfit{x}
    \end{equation*}
\end{definition}
\subsection{Directional Derivatives}
\begin{definition}
    The directional derivative $\symbf{\nabla}_{\symbfit{u}}f$ is the rate at
    which the function $f$ changes in the direction $\symbfit{u}$.
    \begin{align*}
        \symbf{\nabla}_{\symbfit{u}}f & = \lim_{h \to 0} \frac{f(\symbfit{x} + h\symbfit{u}) - f(\symbfit{x})}{h} \\
                                      & = \symbf{\nabla}f \cdot \symbfit{u}
    \end{align*}
    where the slope is given by $\norm{\symbf{\nabla}_{\symbfit{u}}f}$.
\end{definition}
\begin{remark}
    The directional derivative of $f$ can be denoted in several ways:
    \begin{equation*}
        \symbf{\nabla}_{\symbfit{u}}f = D_{\symbfit{u}} f = \partial_{\symbfit{u}} f = \pdv{f}{\symbfit{u}}
    \end{equation*}
\end{remark}
\begin{theorem}[Direction of Greatest Ascent]
    The direction of greatest ascent is given by
    \begin{equation*}
        \max_{\norm{\symbfit{u}} = 1} \symbf{\nabla}_{\symbfit{u}}f = \symbf{\nabla}f
    \end{equation*}
    where the slope is given by $\norm{\symbf{\nabla}f}$.
\end{theorem}
\begin{theorem}[Direction of Greatest Descent]
    The direction of greatest descent is given by
    \begin{equation*}
        \min_{\norm{\symbfit{u}} = 1} \symbf{\nabla}_{\symbfit{u}}f = -\symbf{\nabla}f
    \end{equation*}
    where the slope is given by $-\norm{\symbf{\nabla}f}$.
\end{theorem}
\begin{proof}
    Given that $\symbfit{u}$ is a unit vector, the dot product definition
    gives
    \begin{align}
        \symbf{\nabla}_{\symbfit{u}}f & = \symbf{\nabla}f \cdot \symbfit{u} \nonumber                                       \\
                                      & = \norm{\symbf{\nabla}f} \norm{\symbfit{u}} \cos{\left( \theta \right)} \nonumber   \\
                                      & = \norm{\symbf{\nabla}f} \cos{\left( \theta \right)} \label{directional_derivative}
    \end{align}
    \hyperref[directional_derivative]{Equation \ref{directional_derivative}} is
    maximised when $\cos{\left( \theta \right)}$ is maximised. Thus the
    maximum slope is given by
    \begin{equation*}
        \max \symbf{\nabla}_{\symbfit{u}}f = \norm{\symbf{\nabla}f}
    \end{equation*}
    and the direction of greatest ascent is given by
    \begin{equation*}
        \symbfit{u} = \symbf{\nabla}f
    \end{equation*}
\end{proof}
\begin{theorem}
    If $\symbf{\nabla}f(c_1,\: \ldots,\: c_n) \neq 0$, then $\symbf{\nabla}f(c_1,\: \ldots,\: c_n)$ is normal to the level
    curve of $f$ that passes through $(c_1,\: \ldots,\: c_n)$.
\end{theorem}
\subsection{Higher-Order Partial Derivatives}
\begin{definition}
    Higher-order partial derivatives can be denoted using three different
    notations. The following table shows the mixed partial derivative of
    $f(x,\: y)$ w.r.t. $x$ then $y$.
    \begin{table}[H]
        \centering
        \begin{tabular}{c c c}
            \toprule
            \textbf{Leibniz}             & \textbf{Euler}      & \textbf{Legendre} \\
            \midrule
            $\displaystyle\pdv{f}{y}{x}$ & $\partial_{x y}{f}$ & $f_{x y}$         \\
            \bottomrule
        \end{tabular}
        \caption{Mixed Partial Derivative Notation}
    \end{table}
    For partial derivatives w.r.t. the same variable, a superscript can be used
    in Leibniz and Euler notation.
    \begin{table}[H]
        \centering
        \begin{tabular}{c c c}
            \toprule
            \textbf{Leibniz}             & \textbf{Euler}      & \textbf{Legendre} \\
            \midrule
            $\displaystyle\pdv[2]{f}{x}$ & $\partial^2_{x}{f}$ & $f_{x x}$         \\
            \bottomrule
        \end{tabular}
        \caption{Second-Order Partial Derivative Notation}
        % \label{}
    \end{table}
\end{definition}
\subsection{Hessian Matrix}
\begin{definition}
    Let the Hessian matrix $\symbf{H}$ be the matrix of second-order partial
    derivative operators as shown below
    \begin{equation*}
        \symbf{H} =
        \mqty[
        \partial^2_{x_1} & \cdots & \partial_{x_nx_1} \\
        \vdots & \ddots & \vdots \\
        \partial_{x_1x_n} & \cdots & \partial^2_{x_n}
        ]
    \end{equation*}
    Operating on the function $f(x,\: y)$ gives
    \begin{equation*}
        \symbf{H}_f =
        \mqty[
        \partial^2_{x_1}f & \cdots & \partial_{x_nx_1}f \\
        \vdots & \ddots & \vdots \\
        \partial_{x_1x_n}f & \cdots & \partial^2_{x_n}f
        ]
    \end{equation*}
\end{definition}
\subsection{Critical Points}
For the function $f(x,\: y)$, the point $(x_0,\: y_0)$ is a critical point if
\begin{equation*}
    \symbf{\nabla}f(x_0,\: y_0) = 0
\end{equation*}
or if $\symbf{\nabla}f(x_0,\: y_0)$ is undefined.
\subsection{Classification of Critical Points}
The nature of a critical point can be classified using the second derivative test:
\begin{itemize}
    \item if $\left.\det{\left(\mathbf{H}_f\right)}\right|_{(x_0,\: y_0)} > 0$, then the point is a local minima or maxima
          \begin{itemize}
              \item if $f_{xx}(x_0, \: y_0) < 0$, then the point is a local maxima
              \item if $f_{xx}(x_0, \: y_0) > 0$, then the point is a local minima\footnote[1]{For the local minima/maxima, the second derivative can also be taken w.r.t. $y$.}
          \end{itemize}
    \item if $\left.\det{\left(\mathbf{H}_f\right)}\right|_{(x_0,\: y_0)} < 0$, then the point is a saddle point
    \item if $\left.\det{\left(\mathbf{H}_f\right)}\right|_{(x_0,\: y_0)} = 0$, then the test is inconclusive.
\end{itemize}
\newpage
\section{Double and Triple Integrals}
\subsection{Volume under a Two Variable Function}
\begin{definition}
    If $f$ is a function of two variables that is continuous and nonnegative
    on a region $\Omega$ in the $xy$-plane, then the volume of the solid
    enclosed between the surface $z=f(x,\: y)$ and the region $\Omega$ is
    defined by
    \begin{equation}\label{eq:volume_under_2d_function}
        V = \lim_{n \to \infty} \sum_{k = 1}^n f(x_k^\ast,\: y_k^\ast) \Delta A_k
    \end{equation}
\end{definition}
\begin{proof}
    Using lines parallel to the coordinate axes, the region $\Omega$ can be divided
    into $n$ rectangles, where any rectangles outside $\Omega$ are discarded.
    The area of the $k$th remaining rectangle at the arbitrary point $(x_k^\ast,\: y_k^\ast)$
    is given by $\Delta A_k$. Thus the product $f(x_k^\ast,\: y_k^\ast)\Delta A_k$ is the
    volume of the $k$th rectangular parallelepiped, and the sum of all $n$ volumes over
    the region $\Omega$ approximate the volume $V$ of the entire solid.
\end{proof}
\subsection{Double Integral}
\begin{definition}
    By extension of the definite integral of a single variable function expressed in
    \hyperref[theorem:1d_riemann_sums]{Theorem \ref{theorem:1d_riemann_sums}}, the sums in
    \hyperref[eq:volume_under_2d_function]{Equation \ref{eq:volume_under_2d_function}}
    are also called Riemann sums, and the limit is denoted as
    \begin{equation*}
        \iint\limits_{\Omega} f(x,\: y)  \dd{A}
        = \sum_{k=1}^{\infty} f(x_k^\ast,\: y_k^\ast) \Delta A_k
    \end{equation*}
\end{definition}
\begin{tcolorboxlarge}[title={Properties of Double Integrals}]
    \begin{theorem}
        Suppose that $f(x,\: y) $ and $g(x,\: y)$ are continuous on $\Omega$,
        and $\Omega$ can be subdivided into $\Omega_1$ and $\Omega_2$, then
        \begin{enumerate}[label=\normalfont\alph*)]
            \item $\displaystyle\iint\limits_\Omega kf(x,\: y) \dd{A}
                        = k\iint\limits_\Omega f(x,\: y) \dd{A}$.
            \item $\displaystyle\iint\limits_\Omega f(x,\: y) + g(x,\: y) \dd{A}
                        = \iint\limits_\Omega f(x,\: y) \dd{A} + \iint\limits_\Omega g(x,\: y) \dd{A}$.
            \item $\displaystyle\iint\limits_\Omega f(x,\: y) \dd{A}
                        = \iint\limits_{\Omega_1} f(x,\: y) \dd{A} + \iint\limits_{\Omega_2} f(x,\: y) \dd{A}$.
        \end{enumerate}
    \end{theorem}
\end{tcolorboxlarge}
\subsection{Rectangular Regions}
If $\Omega$ is a region bounded by $a \leq x \leq b$ and $c \leq y \leq d$, then
\begin{equation*}
    \iint\limits_{\Omega} f(x,\: y) \dd{A} = \int_c^d\int_a^b f(x,\: y) \dd{x} \dd{y} = \int_a^b\int_c^d f(x,\: y) \dd{y} \dd{x}
\end{equation*}
\subsection{Nonrectangular Regions}
If the limits of integration depend on the variable $x$ or $y$, then the region
may be classified as Type I or Type II.
\subsubsection{Type I Regions}
If the region is:
\begin{description}
    \item[Bounded on the left \& right by:] $x=a$ and $x=b$
    \item[Bounded below \& above by:] $y=g_1(x)$ and $y=g_2(x)$
\end{description}
where $g_1(x) \leq g_2(x)$ for $a \leq x \leq b$, then
\begin{equation*}
    \iint\limits_{\Omega} f(x,\: y) \dd{A} = \int_a^b\int_{g_1(x)}^{g_2(x)} f(x,\: y) \dd{y} \dd{x}.
\end{equation*}
\subsubsection{Type II Regions}
If the region is:
\begin{description}
    \item[Bounded on the left \& right by:] $x=h_1(y)$ and $x=h_2(y)$
    \item[Bounded below \& above by:] $y=c$ and $y=d$
\end{description}
where $h_1(y) \leq h_2(y)$ for $c \leq y \leq d$, then
\begin{equation*}
    \iint\limits_{\Omega} f(x,\: y) \dd{A} = \int_c^d\int_{h_1(y)}^{h_2(y)} f(x,\: y) \dd{x} \dd{y}.
\end{equation*}
\subsection{Polar Coordinates}
To determine the area of a region defined using polar coordinates,
the function can be integrated w.r.t. the radius $r$, and the angle $\theta$.
\begin{equation*}
    \iint\limits_{\Omega} f(r,\: \theta) \dd{A} = \int_{\theta_1}^{\theta_2}\int_{r_1(\theta)}^{r_2(\theta)} f\left(r,\: \theta\right) r \dd{r} \dd{\theta}.
\end{equation*}
\subsection{Volume of a Three Variable Function}
\begin{definition}
    If $f$ is a function of three variables that is continuous and nonnegative
    on a region $\Omega$ in the $xyz$-space, then the volume enclosed by $f(x,\: y, \: z)$
    and the region $\Omega$ is defined by
    \begin{equation}\label{eq:volume_of_3d_function}
        V = \lim_{n\to\infty} \sum_{k=1}^{n} f(x_k^\ast,\: y_k^\ast,\: z_k^\ast) \Delta V_k
    \end{equation}
\end{definition}
\begin{proof}
    Using planes parallel to the coordinate planes, the
    region $\Omega$ can be divided into $n$ boxes, where
    boxes containing points outside of $\Omega$ are discarded.
    The volume of the $k$th remaining box at the arbitrary point
    $(x_k^\ast,\: y_k^\ast,\: z_k^\ast)$ is $\Delta V_k$.
    Thus the product $f(x_k^\ast,\: y_k^\ast,\: z_k^\ast)\Delta V_k$
    is the volume of the $k$th box, and the sum of all $n$ volumes over
    the region $\Omega$ approximate the volume $V$ of the entire solid.
\end{proof}
\subsection{Triple Integrals}
\begin{definition}
    The triple integral of a function is the net signed volume
    defined over a finite closed solid region $\Omega$, and the sums in
    \hyperref[eq:volume_of_3d_function]{Equation \ref{eq:volume_of_3d_function}}
    are also called Riemann sums, and the limit is denoted as
    \begin{equation*}
        \iiint\limits_{\Omega} f(x,\: y,\: z)  \dd{V}
        = \sum_{k=1}^{\infty} f(x_k^\ast,\: y_k^\ast,\: z_k^\ast) \Delta V_k
    \end{equation*}
\end{definition}
\begin{tcolorboxlarge}[title={Properties of Triple Integrals}]
    \begin{theorem}
        Suppose that $f(x,\: y,\: z) $ and $g(x,\: y,\: z)$ are continuous on $\Omega$,
        and $\Omega$ can be subdivided into $\Omega_1$ and $\Omega_2$ then
        \begin{enumerate}[label=\normalfont\alph*)]
            \item $\displaystyle\iiint\limits_\Omega kf(x,\: y,\: z) \dd{V}
                        = k\iiint\limits_\Omega f(x,\: y,\: z) \dd{V}$.
            \item $\displaystyle\iiint\limits_\Omega f(x,\: y,\: z) + g(x,\: y,\: z) \dd{V}
                        = \iiint\limits_\Omega f(x,\: y,\: z) \dd{V} + \iiint\limits_\Omega g(x,\: y,\: z) \dd{V}$.
            \item $\displaystyle\iiint\limits_\Omega f(x,\: y,\: z) \dd{V}
                        = \iiint\limits_{\Omega_1} f(x,\: y,\: z) \dd{V} + \iiint\limits_{\Omega_2} f(x,\: y,\: z) \dd{V}$.
        \end{enumerate}
    \end{theorem}
\end{tcolorboxlarge}
\newpage
\section{Vector-Valued Functions}
% Conventions:
% Component-form vectors shall be written like $\avec{x(t),\:y(t),\:(z(t)}$
\begin{definition}
    A vector-valued function (VVF) is some function $\symbf{r}$
    with domain $\mathbb{R}$ and codomain $\mathbb{R}^n$.
    For example,
    \begin{equation*}
        \symbf{r}:\mathbb{R}\to\mathbb{R}^3
        :\symbf{r}(t)=\avec[\big]{x(t),\: y(t),\: z(t)}
    \end{equation*}
    is a VVF where $x,\:y,\:z: \mathbb{R}\to\mathbb{R}$.
\end{definition}
\begin{theorem}
    The domain of $\symbf{r}(t)$ is the intersection of the domains
    of its components.
\end{theorem}
\begin{definition}[Orientation]
    The orientation of $\symbf{r}(t)$ is the direction of motion along the
    curve as the value of the parameter increases.
\end{definition}
\subsection{Limits and Continuity}
\begin{theorem}[Limits of VVFs]
    The limit of a VVF is the vector of the limits of its components.
    \begin{equation*}
        \lim_{t\to a} \symbf{r}(t)
        = \avec[\bigg]{\lim_{t\to a} x(t),\: \lim_{t\to a} y(t),\: \lim_{t\to a} z(t)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Continuity of VVFs]
    The VVF $\symbf{r}(t)$ is continuous at $t=a$ iff
    \begin{equation*}
        \lim_{t\to a} \symbf{r}(t) = \symbf{r}(a).
    \end{equation*}
    This follows that a VVF is continuous if each of its components are also continuous.
\end{theorem}
\subsection{Calculus with VVFs}
\begin{theorem}[Derivatives of VVFs]
    The derivative of a VVF is the vector of the derivatives of its components.
    \begin{equation*}
        \dv{t}\symbf{r}(t) = \avec[\Bigg]{\dv{t}x(t),\: \dv{t}y(t),\: \dv{t}z(t)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Integration of VVFs]
    The integral of a VVF is the vector of the integrals of its components.
    \begin{equation*}
        \int\symbf{r}(t)\dd{t}
        = \avec[\Bigg]{\int x(t) \dd{t},\: \int y(t) \dd{t},\: \int z(t) \dd{t}}
    \end{equation*}
\end{theorem}
\begin{remark}
    When integrating a VVF, each component has its own constant of integration.
\end{remark}
\subsection{Parametrising Lines with VVFs}
\begin{definition}[Equation for a Line]
    A line can be expressed as
    \begin{equation*}
        \symbf{l}(t) = \symbfit{P}_0 + t\symbfit{v}
    \end{equation*}
    where the line $\symbf{l}(t)$ passes through the point $\symbfit{P}_0$, and is parallel to the vector $\symbfit{v}$.
\end{definition}
\begin{definition}[Tangent Lines]
    If a VVF $\symbf{r}(t)$ is differentiable at
    $t_0$ and $\symbf{r'}(t_0)\ne\symbf{0}$, the tangent line at
    $t=t_0$ is given by
    \begin{equation*}
        \symbf{l}(t) = \symbf{r}(t_0)+t\symbf{r'}(t_0).
    \end{equation*}
\end{definition}
\begin{remark}
    Higher-order approximations can be determined using Taylor's formula.
\end{remark}
\subsection{Applications of VVFs}
\begin{theorem}[Curve of Intersection]
    A VVF can be used to determine the curve of intersection between two
    surfaces. The method is to choose one of the variables (commonly the first)
    as the parameter, and express the remaining variables in terms of that parameter.

    If the intersection is bounded between two points, the domain can be
    calculated using the component which was parametrised.

    For example, the curve of intersection between
    \begin{align*}
        y = 2x - 4 \quad \text{and} \quad z = 3x - 1
    \end{align*}
    between the points
    \begin{align*}
        \symbfit{P}_1 = (2,\: 0,\: 7) \quad \text{and} \quad \symbfit{P}_2 = (3,\: 2,\: 10)
    \end{align*}
    is given by
    \begin{equation*}
        \symbf{r}(t) = \avec[\big]{t,\: 2t - 4,\: 3t - 1} : 2 \leq t \leq 3.
    \end{equation*}
\end{theorem}
\begin{definition}[Arc Length]
    The arc length $S$ of a smooth continuous VVF $\symbf{r}(t)$, is the distance along $\symbf{r}(t)$
    between $t=a$ and $t=b$, defined by
    \begin{equation*}
        S = \int_a^b \norm{\symbf{r'}(t)} \dd{t}
    \end{equation*}
\end{definition}
\newpage
\section{Differential Equations}
\begin{definition}[Differential Equations]
    A differential equation (DE) is an equation which involves the derivatives of
    one or more unknown functions (called dependent variables), that are with respect
    to one or more independent variables.
\end{definition}
\begin{definition}[Ordinary Differential Equations]
    An ordinary differential equation (ODE) is a differential equation with
    derivatives with respect to a single variable.
\end{definition}
\begin{definition}[Partial Differential Equations]
    A partial differential equation (PDE) is a differential equation with
    derivatives with respect to multiple variables.
\end{definition}
\begin{definition}[Order of Differential Equations]
    The order of a differential equation is the highest derivative in the equation.
\end{definition}
\begin{definition}[Autonomous Differential Equations]
    An autonomous differential equation does not depend explicitly on the independent variable.
\end{definition}
\begin{definition}[Linear Differential Equations]
    A linear differential equation does not have any products of the dependent variable with itself
    or its derivatives. The general form of a linear ODE of order $n$ is
    \begin{equation*}
        a_n(x)y^{\left( n \right)} + a_{n-1}(x)y^{\left( n-1 \right)} + \cdots + a_1(x)y' + a_0(x)y = F(x).
    \end{equation*}
    The dependent variable cannot be composed in another function.
\end{definition}
\subsection{Qualitative Analysis}
With qualitative analysis we aim to understand the behaviour of solutions to the ODE.
By \linebreak computing fixed points, we can draw a phase line diagram, and sketch solution curves.

For the autonomous differential equation
\begin{equation*}
    \dv{y}{t} = f(y)
\end{equation*}
\begin{definition}[Fixed Point]
    A fixed point is the value of $y$ for which $f(y) = 0$.
\end{definition}
\begin{definition}[Stability]
    By analysing the behaviour of $f(y)$ given a perturbation near fixed points,
    we can determine the stability of those fixed points.
    \begin{table}[H]
        \centering
        \begin{tabular}{c | c}
            \toprule
            \textbf{Behaviour in Positive/Negative Directions} & \textbf{Stability} \\
            \midrule
            Both toward fixed point                            & stable             \\
            Both away from fixed point                         & unstable           \\
            One toward and one away from fixed point           & semi-stable        \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{definition}
\begin{definition}[Phase Plane]
    Using the information about the behaviour of $f(y)$ around fixed points, $f(y)$ can be plotted against $y$
    to construct a phase plane diagram.
\end{definition}
\begin{definition}[Phase Line]
    A phase line is the one-dimensional form of a phase plane,
    that shows the limiting behaviour of $y$ as $t \to \infty$.
\end{definition}
\begin{definition}[Sample Solutions]
    Using a phase line, we sketch the behaviour of sample solutions of $f(y)$.
    where the curves asymptote toward stable
    (also semi-stable) fixed points, and diverge from unstable (also semi-stable) fixed points.
\end{definition}
\newpage
\section{First-Order Differential Equations}
\subsection{Directly Integrable ODEs}
For a differential equation of the form
\begin{equation*}
    \dv{y}{x} = f(x)
\end{equation*}
\begin{equation*}
    y(x) = \int f(x) \dd{x}.
\end{equation*}
\subsection{Separable ODEs}
For a differential equation of the form
\begin{equation*}
    \dv{y}{x} = p(x) q(y),
\end{equation*}
a separation of variables followed by an integration w.r.t. $x$ yields an implicit solution.
\begin{equation*}
    \int \frac{1}{q(y)} \dv{y}{x} \dd{x} = \int p(x) \dd{x}.
\end{equation*}
\subsection{Linear ODEs}
For a differential equation of the form
\begin{equation*}
    \dv{y}{x} + p(x)y = q(x)
\end{equation*}
we can use the integrating factor
\begin{equation*}
    I(x) = \e^{\int p(x) \dd{x}}
\end{equation*}
so that
\begin{equation*}
    y(x) = \frac{1}{I(x)} \int I(x) q(x) \dd{x}
\end{equation*}
\subsection{Exact ODEs}
A differential equation of the form
\begin{equation*}
    P(x,\: y) + Q(x,\: y)\dv{y}{x} = 0
\end{equation*}
has the solution
\begin{equation*}
    \Psi(x,\: y) = c,
\end{equation*}
iff it is exact, namely when
\begin{equation*}
    P_y = Q_x
\end{equation*}
where $P = \Psi_x$ and $Q = \Psi_y$. Then
\begin{gather*}
    \Psi(x,\: y) = \int P(x,\: y) \dd{x} + f(y) \\
    \Psi(x,\: y) = \int Q(x,\: y) \dd{y} + g(x)
\end{gather*}
and $f(y)$ and $g(x)$ can be determined by solving these equations simultaneously.
\newpage
\section{Second-Order Differential Equations}
A linear second-order differential equation is of the form
\begin{equation*}
    a_2(x)y'' + a_1(x)y' + a_0(x)y = F(x).
\end{equation*}
\begin{itemize}
    \item If $F(x) = 0$, then the equation is homogeneous.
    \item If $F(x) \neq 0$, then the equation is nonhomogeneous.
\end{itemize}
\begin{definition}[Initial Value Problem]
    An initial value problem specifies the value for $y$ and its derivative at a single value of the independent variable:
    \begin{align*}
        y(x_0) = y_0, \quad y'(x_0) = y_1.
    \end{align*}
\end{definition}
\begin{definition}[Boundary Value Problem]
    A boundary value problem specifies the value for $y$ at two different values of the independent variable:
    \begin{align*}
        y(x_0) = y_0, \quad y(x_1) = y_1.
    \end{align*}
\end{definition}
\begin{theorem}[Superposition Principle]
    Consider the linear homogeneous ODE
    \begin{equation*}
        a_n(x)y^{\left( n \right)} + a_{n-1}(x)y^{\left( n-1 \right)} + \cdots + a_1(x)y' + a_0(x)y = 0.
    \end{equation*}
    If $y_1(x),\: y_2(x),\: \dots ,\: y_n(x)$ are solutions of the differential equation, then
    the linear combination of these solutions
    \begin{equation*}
        y(x) = c_1 y_1(x) + c_2 y_2(x) + \cdots + c_n y_n(x).
    \end{equation*}
    also satisfies the ODE.
\end{theorem}
\begin{theorem}[Fundamental Set of Solutions]
    The $n$th order linear homogeneous ODE with \linebreak continuous coefficients on an open interval $I$,
    has $n$ non-trivial linearly independent solutions that form
    a fundamental set of solutions on $I$.
\end{theorem}
\subsection{Reduction of Order}
Reduction of order is a method for finding a second solution
to an ODE, given a known solution.
The second solution is of the form
\begin{equation*}
    y_2(x) = v\left(x\right) y_1(x).
\end{equation*}
$v(x)$ can be determined by substituting $y_2$ into the ODE.
\subsection{Homogeneous ODEs}
A second-order constant-coefficient homogeneous ODE
\begin{equation*}
    a_2\dv[2]{y}{x} + a_1\dv{y}{x} + a_0y = 0
\end{equation*}
has solutions of the form
\begin{equation*}
    y(x) = \e^{\lambda x}.
\end{equation*}
\subsection{Characteristic Equation}
Substituting this form into the ODE gives the characteristic equation
\begin{equation*}
    a_2\lambda^2 + a_1\lambda + a_0 = 0.
\end{equation*}
This equation has three distinct cases.
\begin{description}
    \item[Real Distinct Roots.] If $a_1^2 > 4a_0a_2$.
    \item[Real Repeated Roots.] If $a_1^2 = 4a_0a_2$.
    \item[Complex Conjugate Roots.] If $a_1^2 < 4a_0a_2$.
\end{description}
\subsubsection{Real Distinct Roots}
For two real and distinct roots, $\lambda_1$ and $\lambda_2$, the general solution is
\begin{equation*}
    y(x) = c_1\e^{\lambda_1 x} + c_2\e^{\lambda_2 x}
\end{equation*}
\subsubsection{Real Repeated Roots}
For the real repeated root, $\lambda$, the general solution is
\begin{equation*}
    y(x) = c_1\e^{\lambda x} + c_2 t\e^{\lambda x}
\end{equation*}
\subsubsection{Complex Conjugate Roots}
For two complex conjugate roots, $\lambda = \alpha \pm \beta i$, the general solution is
\begin{equation*}
    y(x) = \e^{\alpha x}\bigl( c_1\cos{\left( \beta x \right)} + c_2 \sin{\left( \beta x \right)} \bigr)
\end{equation*}
\section{Nonhomogeneous ODEs}
A second-order constant-coefficient nonhomogeneous ODE
\begin{equation*}
    a_2y'' + a_1y' + a_0y = F(x)
\end{equation*}
has a general solutions of the form
\begin{equation*}
    y(x) = y_H(x) + y_P(x).
\end{equation*}
where $y_H(x)$ satisfies
\begin{equation*}
    a_2\dv[2]{y_H}{x} + a_1\dv{y_H}{x} + a_0y_H = 0
\end{equation*}
and $y_P(x)$ satisfies
\begin{equation*}
    a_2\dv[2]{y_P}{x} + a_1\dv{y_P}{x} + a_0y_P = F(x)
\end{equation*}
\subsection{Method of Undetermined Coefficients}
To find the particular solution $y_P$, we must choose a likely
form that it would take. The following table summarises appropriate
forms of $y_P$ based on $F$.
\begin{table}[H]
    \centering
    \begin{tabular}{c | c}
        \toprule
        $F(x)$ form                                                        & $y_P(x)$ guess                                                          \\
        \midrule
        a constant                                                         & A                                                                       \\
        a polynomial of degree $n$                                         & $\displaystyle \sum_{i = 0}^n A_i x^i$                                  \\
        $\e^{kx}$                                                          & $A \e^{kx}$                                                             \\
        $\cos{\left( \omega x \right)}$ or $\sin{\left( \omega x \right)}$ & $A_0 \cos{\left( \omega x \right)} + A_1 \sin{\left( \omega x \right)}$ \\
        \bottomrule
    \end{tabular}
    \caption{Particular solutions for undetermined coefficients.}
    % \label{}
\end{table}
Once the form of the particular solution has been determined, it can be substituted into the
nonhomogeneous ODE, to determine the undetermined coefficients.
\subsection{Special Forms}
\subsubsection{Product of Forms}
If $F(x)$ is a product of the functions shown above, then the
particular solutions are also multiplied together and any constants are simplified.
\subsubsection{Sum of Forms}
If $F(x)$ is a sum of the functions shown above, then the
particular solutions are also added together.
\subsubsection{Linearly Dependent Forms}
If $F(x)$ is similar to any homogenous solution, then by \textit{definition} of
a homogeneous solution, the solution will be $0$. Hence, $y_P$ must be multiplied by $x$
to ensure that the particular solution is linearly independent to the homogeneous solutions,
in order to form a \textit{fundamental set of solutions}.
\subsection{Finding the General Solution}
\begin{enumerate}
    \item Solve $y_H$
    \item Find an appropriate form for $y_P$
    \item Ensure that $y_P$ is linearly independent to the homogeneous solutions
    \item Substitute $y_P$ into the nonhomogeneous ODE and solve for the undetermined coefficients
    \item Find the general solution $y = y_H + y_P$
    \item Apply any initial or boundary conditions
\end{enumerate}
\subsection{Applications of Second-Order ODEs}
\subsubsection{Spring and Mass Systems}
\begin{equation*}
    F = \text{spring force ($F_s$)} + \text{damping force ($F_d$)} + \text{external force ($f(t)$)}
\end{equation*}
\begin{description}
    \item[Newton's Law] $F = m y''$
    \item[Spring force.] $F_s = -k y$
    \item[Damping force.] $F_d = -\gamma y'$
\end{description}
\begin{equation*}
    m y'' + \gamma y' + k y = f(t)
\end{equation*}
\subsubsection{Electrical Circuits}
Current $i$ is defined as the rate of change of charge $q$
\begin{equation*}
    i = \dv{q}{t}
\end{equation*}
The voltage drop across various elements is given below:
\begin{description}
    \item[Voltage drop across a resistor:] $iR$
    \item[Voltage drop across a capacitor:] $q/C$
    \item[Voltage drop across an inductor:] $L \dv{i}{t}$
\end{description}
where $R$ is the resistance measured in Ohms (\si{\ohm}),
$C$ is the capacitance measured in Farads (\si{\farad}),
and $L$ is the inductance measured in Henrys (\si{\henry}).

Kirchhoff's Voltage law states that the sum of voltages around a loop equals 0.
Therefore in an RLC circuit, with a voltage source supplying $v(t)\si{\volt}$,
\begin{align*}
    v(t) - iR - L \dv{i}{t} - \frac{q}{C}      & = 0     \\
    L \dv[2]{q}{t} + R\dv{q}{t} + \frac{1}{C}q & = v(t).
\end{align*}
\end{document}
